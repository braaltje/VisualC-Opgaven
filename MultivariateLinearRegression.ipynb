{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef1aa9f2-0992-40cb-9d2c-bbbebed0b619",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy, math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4db6fc81-f90a-4555-8e92-392211b4b724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load the data\n",
    "def load_house_data():\n",
    "    data = np.loadtxt(\"./Week2/data/houses.txt\", delimiter=',', skiprows=1)\n",
    "    X = data[:, :4]\n",
    "    y = data[:, 4]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b5d0a462-4f60-479e-b0d7-289b1d80915b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, y_train = load_house_data()\n",
    "X_train = np.array([[2104, 5, 1, 45], [1416, 3, 2, 40], [852, 2, 1, 35]])\n",
    "y_train = np.array([460, 232, 178])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e69b7479-3f1c-4d5b-8e3c-42cbf832f557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_init shape: (4,), b_init type: <class 'float'>\n"
     ]
    }
   ],
   "source": [
    "# for demo'ing w and b will be loaded with initial values that are near the optimal. w is a 1-d NumPy vector\n",
    "b_init = 785.1811367994083\n",
    "w_init = np.array([0.39133535, 18.75376741, -53.36032453, -26.42131618])\n",
    "print(f\"w_init shape: {w_init.shape}, b_init type: {type(b_init)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518827f1-48e7-4c1d-abf7-9fb902736e62",
   "metadata": {},
   "source": [
    "The model's prediction with multiple variables is given by the linear model:\n",
    "\n",
    "$$ f_{\\mathbf{w},b}(\\mathbf{x}) =  w_0x_0 + w_1x_1 +... + w_{n-1}x_{n-1} + b \\tag{1}$$\n",
    "or in vector notation:\n",
    "$$ f_{\\mathbf{w},b}(\\mathbf{x}) = \\mathbf{w} \\cdot \\mathbf{x} + b  \\tag{2} $$ \n",
    "where $\\cdot$ is a vector `dot product`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01f00ad9-3d0a-4959-9347-6cfd069b4165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# equation (1), no dot notation\n",
    "def predict_single_loop(x, w, b):\n",
    "    \"\"\"\n",
    "    single predict using linear regression\n",
    "\n",
    "    Args:\n",
    "        x (ndarray): Shape (n,) example with multiple features\n",
    "        w (ndarray): Shape (n,) model parameters\n",
    "        b (scalar): model parameter\n",
    "\n",
    "    Reurns:\n",
    "        p (scalar): prediction\n",
    "    \"\"\"\n",
    "    n = x.shape[0]\n",
    "    p = 0.0\n",
    "    for i in range(n):\n",
    "        p_i = x[i] * w[i]\n",
    "        p = p + p_i\n",
    "    p = p + b\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68f97e67-924c-4561-a270-3fd31b4cf415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# equation 1(), with dot notation\n",
    "def predict(x, w, b):\n",
    "    \"\"\"\n",
    "    single predict using linear regression\n",
    "    Args:\n",
    "        x (ndarray): Shape (n,) example with multiple features\n",
    "        w (ndarray): Shape (n,) model parameters\n",
    "        b (scalar): model parameter\n",
    "\n",
    "    Reurns:\n",
    "        p (scalar): prediction\n",
    "    \"\"\"\n",
    "    p = np.dot(x, w) + b\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd7b3c27-9606-4665-81f2-483093b3bb9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_vec shape (4,), x_vec value [2104    5    1   45]\n",
      "f_wb shape: (), prediction 459.9999976194082\n"
     ]
    }
   ],
   "source": [
    "# make a prediction, using 1 row from the training data\n",
    "x_vec = X_train[0,:]\n",
    "print(f\"x_vec shape {x_vec.shape}, x_vec value {x_vec}\")\n",
    "\n",
    "# make a prediction\n",
    "f_wb = predict(x_vec, w_init, b_init)\n",
    "print(f\"f_wb shape: {f_wb.shape}, prediction {f_wb}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427f6130-795a-4877-b528-2fe2bed8ed9e",
   "metadata": {},
   "source": [
    "# 4 Compute Cost With Multiple Variables\n",
    "The equation for the cost function with multiple variables $J(\\mathbf{w},b)$ is:\n",
    "$$J(\\mathbf{w},b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})^2 \\tag{3}$$ \n",
    "where:\n",
    "$$ f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b  \\tag{4} $$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "732a2956-43e2-43f2-a0f3-ad9ae03fbcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, y, w, b):\n",
    "    \"\"\"\n",
    "    compute cost\n",
    "    Args:\n",
    "        X (ndarray (m, n)): Data, m examples with n features\n",
    "        y (ndarray (m,)): target values\n",
    "        w (ndarray (n,)): model parameters\n",
    "        b (scalar): model parameter\n",
    "\n",
    "    Returns:\n",
    "        cost (scalar): cost\n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "    cost = 0.0\n",
    "    for i in range(m):\n",
    "        f_wb_i = np.dot(X[i], w) + b # (4) above\n",
    "        cost = cost + (f_wb_i - y[i])**2 # summation in (3) above\n",
    "    cost = cost / (2 * m) # division in (3) above\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df157680-8256-4cfb-b85c-50f917feac7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at optimal w: 1.5578904330213735e-12\n"
     ]
    }
   ],
   "source": [
    "cost = compute_cost(X_train, y_train, w_init, b_init)\n",
    "print(f\"Cost at optimal w: {cost}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fc7bd1-e64d-4439-81e4-4ef0682afb39",
   "metadata": {},
   "source": [
    "# Derivates for gradient descent\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)} \\tag{6}  \\\\\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)}) \\tag{7}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4153f9d8-edf7-4ebd-9025-ef3dd156f732",
   "metadata": {},
   "source": [
    "## 5.1 Compute Gradient with Multiple Variables\n",
    "An implementation for calculating the equations (6) and (7) is below. There are many ways to implement this. In this version, there is an\n",
    "- outer loop over all m examples. \n",
    "    - $\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}$ for the example can be computed directly and accumulated\n",
    "    - in a second loop over all n features:\n",
    "        - $\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}$ is computed for each $w_j$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5844d590-1ce8-4b9d-a281-bb0201c42d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(X, y, w, b):\n",
    "    \"\"\"\n",
    "    Computes the gradient for linear regression\n",
    "    Args:\n",
    "        X (ndarray (m,n)): Data, m examples with n features\n",
    "        y (ndarray (n,)): target values\n",
    "        w (ndarray (n,)): model parameters\n",
    "        b (scalar): model parameter\n",
    "    \n",
    "    Returns:\n",
    "        dj_dw (ndarray (n,)): The gradient of the cost w.r.t. parameters w\n",
    "        dj_db (scalar): The gradient of the cost w.r.t. parameter b\n",
    "    \"\"\"\n",
    "    m, n = X.shape # number of examples, number of features\n",
    "    dj_dw = np.zeros((n,))\n",
    "    dj_db = 0.0\n",
    "\n",
    "    for i in range(m):\n",
    "        err = (np.dot(X[i], w) + b) - y[i] # equation (7) above\n",
    "        for j in range(n):\n",
    "            dj_dw[j] = dj_dw[j] + err * X[i, j] # equation (6)\n",
    "        dj_db = dj_db + err\n",
    "    dj_dw = dj_dw / m\n",
    "    dj_db = dj_db / m\n",
    "\n",
    "    return dj_db, dj_dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "edfa6344-94b9-422d-b5ad-74fcb8ffe0d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dj_db at initial w, b: -1.6739251122999121e-06\n",
      "dj_dw at initial w, b: [-0.00272624 -0.00000627 -0.00000222 -0.00006924]\n"
     ]
    }
   ],
   "source": [
    "# compute and display the gradient\n",
    "tmp_dj_db, tmp_dj_dw = compute_gradient(X_train, y_train, w_init, b_init)\n",
    "print(f\"dj_db at initial w, b: {tmp_dj_db}\")\n",
    "print(f\"dj_dw at initial w, b: {tmp_dj_dw}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afde8e3-cd1e-49e7-8372-c26142d070d4",
   "metadata": {},
   "source": [
    "## Gradient Descent With Multiple Variables\n",
    "\n",
    "Gradient descent for multiple variables:\n",
    "\n",
    "$$\\begin{align*} \\text{repeat}&\\text{ until convergence:} \\; \\lbrace \\newline\\;\n",
    "& w_j = w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} \\tag{5}  \\; & \\text{for j = 0..n-1}\\newline\n",
    "&b\\ \\ = b -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  \\newline \\rbrace\n",
    "\\end{align*}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c706b5ff-af23-4963-bca0-f24fcbe3bb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters):\n",
    "    \"\"\"\n",
    "    Performs batch gradient descent to learn w and b. Updates w and b by taking\n",
    "    num_iuters gradient steps with learning rate alpha.\n",
    "\n",
    "    Args:\n",
    "        X (ndarray (m,n))   : Data, m examples with n features\n",
    "        y (ndarray (m,))    : Target values\n",
    "        w_in (ndarray (n,)) : initial model parameters\n",
    "        b_in (scalar)       : initial model parameter\n",
    "        cost_function       : function to compute cost\n",
    "        gradient_function   : function to compute the gradient\n",
    "        alpha (float)       : learning rate\n",
    "        num_iters (int)     : number of iterations\n",
    "\n",
    "    Returns:\n",
    "        w (ndarray (n,)) : Updated values of the parameters\n",
    "        b (scalar)       : Updated value of the parameter\n",
    "    \"\"\"\n",
    "    # an array to store cost J and w's at each iteration for graphing later\n",
    "    J_history = []\n",
    "    w = copy.deepcopy(w_in) # deepcopy to avoid modifying global w\n",
    "    b = b_in\n",
    "\n",
    "    for i in range(num_iters):\n",
    "        # calculate the gradient and update the parameters\n",
    "        dj_db, dj_dw = gradient_function(X, y, w, b)\n",
    "\n",
    "        # update parameters using w, b, alpha and grdient\n",
    "        w =  w - alpha * dj_dw\n",
    "        b = b - alpha * dj_db\n",
    "\n",
    "        # save the cost J at each iteration\n",
    "        if i < 100000: # to prevent resource exhaustion\n",
    "            J_history.append(cost_function(X, y, w, b))\n",
    "\n",
    "        # print cost every at 10 intervals 10 times or as many iterations if < 10\n",
    "        if i % math.ceil(num_iters / 10) == 0:\n",
    "            print(f\"Iteration {i:4d}: Cost {J_history[-1]:8.2f}   \")\n",
    "\n",
    "    return w, b, J_history # J_history for graphing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0638a0b3-dec2-4d39-9bcc-9277c1527545",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0: Cost  2529.46   \n",
      "Iteration  100: Cost   695.99   \n",
      "Iteration  200: Cost   694.92   \n",
      "Iteration  300: Cost   693.86   \n",
      "Iteration  400: Cost   692.81   \n",
      "Iteration  500: Cost   691.77   \n",
      "Iteration  600: Cost   690.73   \n",
      "Iteration  700: Cost   689.71   \n",
      "Iteration  800: Cost   688.70   \n",
      "Iteration  900: Cost   687.69   \n",
      "b, w found by gradient descent: -0.00, [ 0.20396569  0.00374919 -0.0112487  -0.0658614 ] \n",
      "prediction: 426.19, target value: 460\n",
      "prediction: 286.17, target value: 232\n",
      "prediction: 171.47, target value: 178\n"
     ]
    }
   ],
   "source": [
    "# initialize parameters\n",
    "initial_w = np.zeros_like(w_init)\n",
    "initial_b = 0.\n",
    "# some gradient descent settings\n",
    "iterations = 1000\n",
    "alpha = 0.00000050\n",
    "# run gradient descent\n",
    "w_final, b_final, J_hist = gradient_descent(X_train, y_train, initial_w, initial_b,\n",
    "                                            compute_cost, compute_gradient,\n",
    "                                            alpha, iterations)\n",
    "print(f\"b, w found by gradient descent: {b_final:0.2f}, {w_final} \")\n",
    "m, _ = X_train.shape\n",
    "for i in range(m):\n",
    "    print(f\"prediction: {np.dot(X_train[i], w_final) + b_final:0.2f}, target value: {y_train[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c2cc0d-3886-438e-b3bc-7a0666b8ca30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
